{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on News Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been cleaned, we can proceed with trying to train a classifier. The general plan is to use word embeddings from a Word2Vec model to represent the text for each article, then use a many-to-one recurrent neural network to predict the sentiment of these articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the Headlines and Descriptions\n",
    "\n",
    "Right now, the headlines and descriptions are two separate sequences of words that we can't directly analyze. To make things easier, let's concatenate the headlines and descriptions into one block of text so we can treat them as one sequence for the RNN to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv('./data/cleaned_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_combined_column(articles):\n",
    "    '''For each article in the dataframe, concatenate the headline and description into a combined entry,\n",
    "    ignoring NaN entries.'''\n",
    "    for i, row in articles.iterrows():\n",
    "        headline, description = row['headline'], row['description']\n",
    "        combined = \"\"\n",
    "        if type(headline) == str:\n",
    "            combined += headline\n",
    "        elif type(description) == str:\n",
    "            combined += description\n",
    "            \n",
    "        articles.loc[i, 'combined'] = combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>url</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "      <th>source</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Sad Bulldog, A Happy Prince And More Things ...</td>\n",
       "      <td>https://www.huffpost.com/entry/coronavirus-dis...</td>\n",
       "      <td>A sad bulldog and a happy, paint-covered princ...</td>\n",
       "      <td>good</td>\n",
       "      <td>huffpost</td>\n",
       "      <td>A Sad Bulldog, A Happy Prince And More Things ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John Krasinski Shocks 9-Year-Old 'Hamilton' Fa...</td>\n",
       "      <td>https://www.huffpost.com/entry/john-krasinski-...</td>\n",
       "      <td>\"The Office\" star struck gold again in his You...</td>\n",
       "      <td>good</td>\n",
       "      <td>huffpost</td>\n",
       "      <td>John Krasinski Shocks 9-Year-Old 'Hamilton' Fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I Was Struggling As A Single Mom. Then A Stran...</td>\n",
       "      <td>https://www.huffpost.com/entry/struggling-sing...</td>\n",
       "      <td>\"With this gift, I was finally able to get out...</td>\n",
       "      <td>good</td>\n",
       "      <td>huffpost</td>\n",
       "      <td>I Was Struggling As A Single Mom. Then A Stran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pink's Advice To Fans: 'Change The F**king Wor...</td>\n",
       "      <td>https://www.huffpost.com/entry/pink-peoples-ch...</td>\n",
       "      <td>“I care about decency and humanity and kindnes...</td>\n",
       "      <td>good</td>\n",
       "      <td>huffpost</td>\n",
       "      <td>Pink's Advice To Fans: 'Change The F**king Wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 Books For Parents Who Want To Raise Kind Kids</td>\n",
       "      <td>https://www.huffpost.com/entry/parenting-books...</td>\n",
       "      <td>These parenting books emphasize emotional inte...</td>\n",
       "      <td>good</td>\n",
       "      <td>huffpost</td>\n",
       "      <td>10 Books For Parents Who Want To Raise Kind Kids</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  A Sad Bulldog, A Happy Prince And More Things ...   \n",
       "1  John Krasinski Shocks 9-Year-Old 'Hamilton' Fa...   \n",
       "2  I Was Struggling As A Single Mom. Then A Stran...   \n",
       "3  Pink's Advice To Fans: 'Change The F**king Wor...   \n",
       "4   10 Books For Parents Who Want To Raise Kind Kids   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.huffpost.com/entry/coronavirus-dis...   \n",
       "1  https://www.huffpost.com/entry/john-krasinski-...   \n",
       "2  https://www.huffpost.com/entry/struggling-sing...   \n",
       "3  https://www.huffpost.com/entry/pink-peoples-ch...   \n",
       "4  https://www.huffpost.com/entry/parenting-books...   \n",
       "\n",
       "                                         description category    source  \\\n",
       "0  A sad bulldog and a happy, paint-covered princ...     good  huffpost   \n",
       "1  \"The Office\" star struck gold again in his You...     good  huffpost   \n",
       "2  \"With this gift, I was finally able to get out...     good  huffpost   \n",
       "3  “I care about decency and humanity and kindnes...     good  huffpost   \n",
       "4  These parenting books emphasize emotional inte...     good  huffpost   \n",
       "\n",
       "                                            combined  \n",
       "0  A Sad Bulldog, A Happy Prince And More Things ...  \n",
       "1  John Krasinski Shocks 9-Year-Old 'Hamilton' Fa...  \n",
       "2  I Was Struggling As A Single Mom. Then A Stran...  \n",
       "3  Pink's Advice To Fans: 'Change The F**king Wor...  \n",
       "4   10 Books For Parents Who Want To Raise Kind Kids  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_combined_column(articles)\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Before we can represent this new 'combined' entry as a matrix, we have to remove punctuation / special characters and numbers, since they don't convey much meaningful information about sentiment and could bog down the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "def process_text(text):\n",
    "    text = text.lower()\n",
    "    # remove any non alphanumeric characters,\n",
    "    # escaping ' and * because of contractions and swear censorship (e.g. f**k)\n",
    "    text = re.sub(r\"[^a-zA-Z'\\*]\", \" \", text)\n",
    "    # tokenize the combined text by splitting on spaces and removing whitespace\n",
    "    text = text.split()\n",
    "    word_list = []\n",
    "    for i, word in enumerate(text):\n",
    "        # remove any censored swears which would likely just waste space in the w2v model\n",
    "        if '*' in word:\n",
    "            continue\n",
    "        # remove any artifact of the regex checking that leaves a single apostrophe as a 'word'\n",
    "        elif word == \"'\":\n",
    "            continue\n",
    "        # remove single-quoted text by checking for first character apostrophes then last character apostrophes\n",
    "        word = re.sub(r\"^'\", \"\", word)\n",
    "        word = re.sub(r\"'$\", \"\", word)\n",
    "        \n",
    "        # removing 's possessives\n",
    "        word = re.sub(r\"'s$\", \"\", word)\n",
    "        word_list.append(word)\n",
    "        \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>url</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "      <th>source</th>\n",
       "      <th>combined</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Sad Bulldog, A Happy Prince And More Things ...</td>\n",
       "      <td>https://www.huffpost.com/entry/coronavirus-dis...</td>\n",
       "      <td>A sad bulldog and a happy, paint-covered princ...</td>\n",
       "      <td>good</td>\n",
       "      <td>huffpost</td>\n",
       "      <td>A Sad Bulldog, A Happy Prince And More Things ...</td>\n",
       "      <td>[a, sad, bulldog, a, happy, prince, and, more,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John Krasinski Shocks 9-Year-Old 'Hamilton' Fa...</td>\n",
       "      <td>https://www.huffpost.com/entry/john-krasinski-...</td>\n",
       "      <td>\"The Office\" star struck gold again in his You...</td>\n",
       "      <td>good</td>\n",
       "      <td>huffpost</td>\n",
       "      <td>John Krasinski Shocks 9-Year-Old 'Hamilton' Fa...</td>\n",
       "      <td>[john, krasinski, shocks, year, old, hamilton,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I Was Struggling As A Single Mom. Then A Stran...</td>\n",
       "      <td>https://www.huffpost.com/entry/struggling-sing...</td>\n",
       "      <td>\"With this gift, I was finally able to get out...</td>\n",
       "      <td>good</td>\n",
       "      <td>huffpost</td>\n",
       "      <td>I Was Struggling As A Single Mom. Then A Stran...</td>\n",
       "      <td>[i, was, struggling, as, a, single, mom, then,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pink's Advice To Fans: 'Change The F**king Wor...</td>\n",
       "      <td>https://www.huffpost.com/entry/pink-peoples-ch...</td>\n",
       "      <td>“I care about decency and humanity and kindnes...</td>\n",
       "      <td>good</td>\n",
       "      <td>huffpost</td>\n",
       "      <td>Pink's Advice To Fans: 'Change The F**king Wor...</td>\n",
       "      <td>[pink, advice, to, fans, change, the, world]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 Books For Parents Who Want To Raise Kind Kids</td>\n",
       "      <td>https://www.huffpost.com/entry/parenting-books...</td>\n",
       "      <td>These parenting books emphasize emotional inte...</td>\n",
       "      <td>good</td>\n",
       "      <td>huffpost</td>\n",
       "      <td>10 Books For Parents Who Want To Raise Kind Kids</td>\n",
       "      <td>[books, for, parents, who, want, to, raise, ki...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  A Sad Bulldog, A Happy Prince And More Things ...   \n",
       "1  John Krasinski Shocks 9-Year-Old 'Hamilton' Fa...   \n",
       "2  I Was Struggling As A Single Mom. Then A Stran...   \n",
       "3  Pink's Advice To Fans: 'Change The F**king Wor...   \n",
       "4   10 Books For Parents Who Want To Raise Kind Kids   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.huffpost.com/entry/coronavirus-dis...   \n",
       "1  https://www.huffpost.com/entry/john-krasinski-...   \n",
       "2  https://www.huffpost.com/entry/struggling-sing...   \n",
       "3  https://www.huffpost.com/entry/pink-peoples-ch...   \n",
       "4  https://www.huffpost.com/entry/parenting-books...   \n",
       "\n",
       "                                         description category    source  \\\n",
       "0  A sad bulldog and a happy, paint-covered princ...     good  huffpost   \n",
       "1  \"The Office\" star struck gold again in his You...     good  huffpost   \n",
       "2  \"With this gift, I was finally able to get out...     good  huffpost   \n",
       "3  “I care about decency and humanity and kindnes...     good  huffpost   \n",
       "4  These parenting books emphasize emotional inte...     good  huffpost   \n",
       "\n",
       "                                            combined  \\\n",
       "0  A Sad Bulldog, A Happy Prince And More Things ...   \n",
       "1  John Krasinski Shocks 9-Year-Old 'Hamilton' Fa...   \n",
       "2  I Was Struggling As A Single Mom. Then A Stran...   \n",
       "3  Pink's Advice To Fans: 'Change The F**king Wor...   \n",
       "4   10 Books For Parents Who Want To Raise Kind Kids   \n",
       "\n",
       "                                           processed  \n",
       "0  [a, sad, bulldog, a, happy, prince, and, more,...  \n",
       "1  [john, krasinski, shocks, year, old, hamilton,...  \n",
       "2  [i, was, struggling, as, a, single, mom, then,...  \n",
       "3       [pink, advice, to, fans, change, the, world]  \n",
       "4  [books, for, parents, who, want, to, raise, ki...  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles['processed'] = articles['combined'].apply(lambda text: process_text(text))\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Training the Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize a Word2Vec Model with 300-dimensional word vectors and a subsampling parameter of 1e-5, since [Mikolov et al](https://arxiv.org/pdf/1310.4546.pdf) achieved good results with these settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(size = 300, sample=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I pulled a [pre-existing dataset](https://www.kaggle.com/therohk/million-headlines) of news headlines that we can add to the dataset I assembled to give our model as robust and wide a vocabulary as possible. Let's load that and have the word2vec model build a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc_headlines = pd.read_csv('./data/abc_articles.csv')\n",
    "abc_headlines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, process the headlines the same way as previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>[aba, decides, against, community, broadcastin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>[act, fire, witnesses, must, be, aware, of, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>[a, g, calls, for, infrastructure, protection,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>[air, nz, staff, in, aust, strike, for, pay, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>[air, nz, strike, to, affect, australian, trav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text  \\\n",
       "0      20030219  aba decides against community broadcasting lic...   \n",
       "1      20030219     act fire witnesses must be aware of defamation   \n",
       "2      20030219     a g calls for infrastructure protection summit   \n",
       "3      20030219           air nz staff in aust strike for pay rise   \n",
       "4      20030219      air nz strike to affect australian travellers   \n",
       "\n",
       "                                           processed  \n",
       "0  [aba, decides, against, community, broadcastin...  \n",
       "1  [act, fire, witnesses, must, be, aware, of, de...  \n",
       "2  [a, g, calls, for, infrastructure, protection,...  \n",
       "3  [air, nz, staff, in, aust, strike, for, pay, r...  \n",
       "4  [air, nz, strike, to, affect, australian, trav...  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc_headlines['processed'] = abc_headlines['headline_text'].apply(lambda text: process_text(text))\n",
    "abc_headlines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, convert all the rows into the format that the gensim Word2Vec model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_rows = abc_headlines['processed'].to_list()\n",
    "\n",
    "article_rows = articles['processed'].to_list()\n",
    "\n",
    "combined_rows = abc_rows + article_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the vocabulary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.24 mins\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "w2v_model.build_vocab(combined_rows)\n",
    "\n",
    "current = time.time()\n",
    "elapsed = (current - start) / 60\n",
    "print('Time to build vocab: {} mins'.format(round(elapsed, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, train it on the *combined* rows because the more data we have here, the better the model will be able to discern the relationships between the words. The sentiment classification will be done by the RNN later, so for right now we just want the best embeddings we can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train model: 4.48 mins\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "w2v_model.train(\n",
    "    combined_rows, \n",
    "    total_examples = w2v_model.corpus_count, \n",
    "    epochs = 30)\n",
    "\n",
    "current = time.time()\n",
    "elapsed = (current - start) / 60\n",
    "print('Time to train model: {} mins'.format(round(elapsed, 2)))\n",
    "\n",
    "w2v_model.save('./models/w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to sanity check the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pandemic', 0.6704552173614502),\n",
       " ('vaccinated', 0.5635499358177185),\n",
       " ('streaming', 0.554112434387207),\n",
       " ('updates', 0.5361689329147339),\n",
       " ('antibodies', 0.5320745706558228),\n",
       " ('ebola', 0.5311349630355835),\n",
       " ('coronavirus', 0.5284198522567749),\n",
       " ('neighbors', 0.5263728499412537),\n",
       " ('vaccinate', 0.5231813192367554),\n",
       " ('swine', 0.5097109079360962)]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('covid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it did pretty okay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_embed(word_list, word_vectors):\n",
    "    word_tensors = []\n",
    "    num_words = len(word_list)\n",
    "    for word in word_list:\n",
    "        try:\n",
    "            word_vector = word_vectors.get_vector(word)\n",
    "            word_torch_tensor = torch.from_numpy(word_vector)\n",
    "            word_tensors.append(word_torch_tensor)\n",
    "        except KeyError as e:\n",
    "            print(e, 'Ignoring word.')\n",
    "            num_words -= 1\n",
    "            pass\n",
    "    # convert the list of tensors into one long tensor with shape (len(word_list), 300)\n",
    "    article_tensor = torch.stack(word_tensors)\n",
    "    article_tensor = article_tensor.view(article_tensor.shape[0], 300, 1)\n",
    "    return article_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pink', 'says', 'world']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = remove_stopwords(\"Pink says f**k the world!\")\n",
    "# remove any non alphanumeric characters,\n",
    "# escaping ' and * because of contractions and swear censorship (e.g. f**k)\n",
    "text = re.sub(r\"[^\\w'\\* ]\", \" \", text)\n",
    "word_list = text.split()\n",
    "word_list = [word for word in word_list if '*' not in word]\n",
    "\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = articles['description'].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(x):\n",
    "    if item == True:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('./google-news-w2v.gz', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.25976562e-01, -1.43554688e-01,  3.61328125e-01,  4.57031250e-01,\n",
       "       -2.12890625e-01,  2.05078125e-01,  2.75390625e-01, -1.44531250e-01,\n",
       "        2.63671875e-01,  2.87109375e-01, -1.27929688e-01, -1.05468750e-01,\n",
       "       -3.44238281e-02, -9.52148438e-03, -6.78710938e-02,  2.91015625e-01,\n",
       "        3.65234375e-01, -8.23974609e-03, -1.54296875e-01, -1.27929688e-01,\n",
       "       -1.31835938e-02,  1.02539062e-01,  1.06933594e-01,  1.47460938e-01,\n",
       "       -1.80816650e-03,  9.32617188e-02,  1.03759766e-02,  1.56250000e-01,\n",
       "       -4.00390625e-02,  6.54296875e-02,  1.75781250e-02,  6.49414062e-02,\n",
       "       -1.02050781e-01, -1.49536133e-02,  2.52685547e-02,  6.59179688e-02,\n",
       "        5.81054688e-02,  9.96093750e-02,  1.01562500e-01,  3.61328125e-02,\n",
       "        1.79687500e-01,  2.95410156e-02,  1.25000000e-01, -1.80244446e-04,\n",
       "       -5.44433594e-02,  2.30468750e-01,  1.52343750e-01, -1.77734375e-01,\n",
       "        6.12792969e-02, -1.20605469e-01, -8.54492188e-02,  2.25585938e-01,\n",
       "       -2.23632812e-01,  9.76562500e-02,  1.36718750e-01, -2.57568359e-02,\n",
       "        1.78710938e-01, -7.86132812e-02,  1.61132812e-02, -1.03515625e-01,\n",
       "        1.48437500e-01,  2.29492188e-01, -4.78515625e-02, -1.41601562e-01,\n",
       "        2.68554688e-02, -3.37890625e-01, -2.53906250e-02,  2.35351562e-01,\n",
       "       -9.61914062e-02,  6.34765625e-02,  1.36718750e-01,  1.42578125e-01,\n",
       "       -2.11181641e-02,  1.47460938e-01, -8.34960938e-02, -1.85546875e-01,\n",
       "       -8.59375000e-02,  2.83203125e-02, -7.12890625e-02,  6.00585938e-02,\n",
       "       -7.78198242e-03, -3.02734375e-01,  2.52685547e-02,  1.95312500e-01,\n",
       "       -1.86523438e-01,  2.18750000e-01,  1.99218750e-01,  3.20312500e-01,\n",
       "       -7.03125000e-02, -1.36108398e-02, -2.04467773e-03,  6.83593750e-02,\n",
       "        3.49121094e-02,  3.83300781e-02, -1.08886719e-01,  9.08203125e-02,\n",
       "        6.49414062e-02,  2.10937500e-01,  3.70788574e-03,  1.25000000e-01,\n",
       "       -1.31835938e-01, -1.44531250e-01, -1.40625000e-01,  1.72851562e-01,\n",
       "       -1.25000000e-01, -9.27734375e-02, -1.38671875e-01, -1.33666992e-02,\n",
       "        1.97265625e-01, -7.32421875e-02, -3.80859375e-02, -7.03125000e-02,\n",
       "        3.66210938e-02,  1.33789062e-01,  1.86523438e-01,  1.33056641e-02,\n",
       "        1.58203125e-01,  7.86132812e-02, -1.13525391e-02,  2.20947266e-02,\n",
       "        2.18505859e-02, -8.49609375e-02, -9.57031250e-02, -1.63085938e-01,\n",
       "        1.35742188e-01,  3.08227539e-03, -1.37695312e-01,  2.46093750e-01,\n",
       "        5.78613281e-02,  1.91650391e-02, -1.52343750e-01,  7.47070312e-02,\n",
       "       -1.87500000e-01,  1.07421875e-01, -5.61523438e-02,  4.27246094e-02,\n",
       "        2.45117188e-01,  1.90429688e-02,  1.49536133e-02,  1.54296875e-01,\n",
       "        2.67578125e-01, -1.66992188e-01,  1.48437500e-01,  1.14746094e-02,\n",
       "        2.18750000e-01, -1.60980225e-03,  1.48437500e-01,  9.47265625e-02,\n",
       "        3.14941406e-02, -1.29882812e-01,  1.19140625e-01, -1.97265625e-01,\n",
       "       -2.26562500e-01, -2.51953125e-01, -8.44726562e-02, -2.78320312e-02,\n",
       "        2.46582031e-02,  1.73828125e-01, -3.04687500e-01, -8.83789062e-02,\n",
       "       -1.03027344e-01,  3.03955078e-02, -7.04956055e-03,  3.01513672e-02,\n",
       "       -2.03125000e-01, -1.63085938e-01, -9.76562500e-02,  8.69140625e-02,\n",
       "       -2.07031250e-01, -3.83300781e-02, -2.92968750e-01,  5.24902344e-02,\n",
       "       -4.76074219e-02, -1.89453125e-01, -7.41577148e-03,  1.25976562e-01,\n",
       "       -2.83203125e-02, -1.48437500e-01, -4.61425781e-02,  7.08007812e-02,\n",
       "       -4.73632812e-02, -1.17187500e-01, -2.50244141e-02,  1.25976562e-01,\n",
       "        5.51757812e-02,  1.77734375e-01, -2.13867188e-01, -2.27539062e-01,\n",
       "       -8.91113281e-03,  2.10937500e-01,  3.06640625e-01, -2.01171875e-01,\n",
       "        8.64257812e-02,  8.05664062e-02,  1.04980469e-01,  3.58886719e-02,\n",
       "       -2.45117188e-01,  5.71289062e-02,  5.37109375e-02,  1.56250000e-01,\n",
       "       -1.60156250e-01,  2.39257812e-01, -1.46484375e-01, -8.98437500e-02,\n",
       "       -1.97265625e-01, -1.25976562e-01, -2.34375000e-01, -6.93359375e-02,\n",
       "       -7.71484375e-02,  3.68652344e-02, -4.94384766e-03,  1.17675781e-01,\n",
       "        2.25830078e-02,  1.12792969e-01, -2.19726562e-01,  9.32617188e-02,\n",
       "       -2.67333984e-02,  2.18505859e-02, -7.91015625e-02, -2.67578125e-01,\n",
       "       -2.22656250e-01, -2.56347656e-02,  6.39648438e-02, -2.67333984e-02,\n",
       "        5.34667969e-02, -3.49121094e-02, -4.46777344e-02,  1.23535156e-01,\n",
       "       -1.03027344e-01,  2.11181641e-02,  2.02148438e-01, -1.26953125e-01,\n",
       "        1.40380859e-02,  1.49414062e-01,  1.62109375e-01,  1.33789062e-01,\n",
       "        1.76757812e-01,  1.62109375e-01,  5.56640625e-02,  1.34277344e-02,\n",
       "       -2.22167969e-02,  3.96728516e-03,  3.14453125e-01, -7.95898438e-02,\n",
       "       -5.81054688e-02,  6.78710938e-02,  2.16796875e-01,  1.05957031e-01,\n",
       "       -1.75781250e-01, -3.90625000e-02,  3.24707031e-02, -8.30078125e-02,\n",
       "        1.67968750e-01,  4.29687500e-02, -4.22363281e-02, -1.78710938e-01,\n",
       "        8.88671875e-02,  1.62109375e-01,  3.54003906e-02,  1.96289062e-01,\n",
       "       -9.91821289e-04,  2.22167969e-02, -1.70898438e-01,  1.63085938e-01,\n",
       "       -8.00781250e-02,  2.21679688e-01, -1.83593750e-01, -1.04003906e-01,\n",
       "       -2.75390625e-01, -7.12890625e-02,  2.83203125e-02,  2.47070312e-01,\n",
       "        2.55859375e-01,  1.33789062e-01,  2.59399414e-03, -1.73339844e-02,\n",
       "       -2.94189453e-02,  1.85546875e-02,  9.09423828e-03,  5.95703125e-02,\n",
       "        5.66406250e-02, -2.67578125e-01,  1.62109375e-01,  9.13085938e-02,\n",
       "       -1.66015625e-01, -5.56640625e-02,  2.19726562e-02, -1.14746094e-01,\n",
       "        2.28271484e-02, -8.88671875e-02,  7.95898438e-02,  2.44140625e-02,\n",
       "       -2.92968750e-02, -6.22558594e-02, -1.80053711e-03, -2.67578125e-01,\n",
       "       -1.13281250e-01, -5.68847656e-02, -5.61523438e-02, -3.40270996e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.get_vector(\"shouldn't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        self.embeddings = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
