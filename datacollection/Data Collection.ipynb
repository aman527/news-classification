{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scrape data from the Good News Network website for, well, good news, and from huffpost for both good and bad news. This should give us a reasonably sized dataset of labeled examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q bs4 requests grequests lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/grequests.py:22: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.contrib.pyopenssl (/opt/conda/lib/python3.7/site-packages/urllib3/contrib/pyopenssl.py)', 'urllib3.util (/opt/conda/lib/python3.7/site-packages/urllib3/util/__init__.py)']. \n",
      "  curious_george.patch_all(thread=False, select=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# goodnews network and huffpost are two helper modules I've created to make scraping the sites much easier\n",
    "import goodnewsnetwork\n",
    "import huffpost\n",
    "import newser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting max results to 10,000 for both good and bad news from HuffPost should hopefully catch all the articles. It seems unlikely that huffpost has written and categorized that many articles for both good and bad news, especially considering that they were launched in 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 250 pages from Good News Network, now adding descriptions.\n",
      "\n",
      "Expected articles_per_request to be a multiple of 3, since articles are returned by newser.com in groups of three. Fetching 501 articles instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "huff_good = huffpost.scrape(category = 'good-news', max_results = 10000)\n",
    "huff_bad = huffpost.scrape(category = 'bad-news', max_results = 10000)\n",
    "\n",
    "# scrape the first 250 pages of goodnewsnetwork.org (the first 5000 articles)\n",
    "gnn_articles = goodnewsnetwork.scrape(stop = 250)\n",
    "\n",
    "newser_articles = newser.scrape(num_requests = 10, articles_per_request = 500)\n",
    "\n",
    "articles = huff_good + huff_bad + gnn_articles + newser_articles\n",
    "\n",
    "articles_dataframe = pd.DataFrame(articles)\n",
    "\n",
    "articles_dataframe.to_csv('../data/articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's scrape some articles from Huffpost's [Impact](https://www.huffpost.com/impact/) category, which is \"dedicated to causes, actionable news, inspiring stories and solutions of all scales,\" according to their LinkedIn Page. It's safe to say that we can label these as good news. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_dataframe = pd.read_csv('../data/articles.csv')\n",
    "\n",
    "huff_impact = huffpost.scrape(category = 'impact', max_results = 10000)\n",
    "\n",
    "for article in huff_impact:\n",
    "    '''the way the algorithm is written, the huffpost scraper sets the sentiment category to \n",
    "    the value of the *news* category passed to it. This replaces the category 'impact' with 'good'\n",
    "    for consistency.'''\n",
    "    article['category'] = 'good'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_df = pd.DataFrame(huff_impact)\n",
    "\n",
    "articles_dataframe = articles_dataframe.append(impact_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To round out the dataset, let's grab some neutral articles. After looking through Huffpost, the Style & Beauty category and the Home & Living category seem to be fairly neutral, with mostly informational content. Without a human labeling the data, it's tough to identify neutral articles, so this may be the best we can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_and_beauty = huffpost.scrape(category = 'style-beauty', max_results = 10000)\n",
    "home_and_living = huffpost.scrape(category = 'home-living', max_results = 10000)\n",
    "\n",
    "neutral_articles = style_and_beauty + home_and_living\n",
    "for article in neutral_articles:\n",
    "    article['category'] = 'neutral'\n",
    "    \n",
    "neutral_dataframe = pd.DataFrame(neutral_articles)\n",
    "\n",
    "articles_dataframe = articles_dataframe.append(neutral_dataframe)\n",
    "\n",
    "articles_dataframe.to_csv('../data/articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neutral_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we could definitely use some more neutral articles. Also, clearly we mislabled some of the bad articles as good, but we can handle that later in the data cleaning notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_dataframe = pd.read_csv('../data/articles.csv')\n",
    "\n",
    "weird_news = huffpost.scrape(category = 'weird-news', max_results = 10000)\n",
    "\n",
    "food_drink = huffpost.scrape(category = 'food-drink', max_results = 10000)\n",
    "\n",
    "neutrals = weird_news + food_drink\n",
    "\n",
    "for article in neutrals:\n",
    "    article['category'] = 'neutral'\n",
    "    \n",
    "neutral_df = pd.DataFrame(neutrals)\n",
    "\n",
    "articles_dataframe = articles_dataframe.append(neutral_df)\n",
    "\n",
    "articles_dataframe.to_csv('../data/articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neutral_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so that got us a *few* more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv('../data/articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "good       10901\n",
       "neutral      523\n",
       "bad          453\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may have to live with it. We can try some augmentation later if really necessary but otherwise, let's just move on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
